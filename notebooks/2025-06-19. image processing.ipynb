{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=4\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 누락된 import 추가\n",
    "from collections import deque\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcw/miniconda3/envs/openvla-oft/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-30 22:03:52.143005: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-30 22:03:52.180928: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-30 22:03:52.180965: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-30 22:03:52.182267: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-30 22:03:52.189664: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-30 22:03:53.096490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LIBERO constants:\n",
      "  NUM_ACTIONS_CHUNK = 8\n",
      "  ACTION_DIM = 7\n",
      "  PROPRIO_DIM = 8\n",
      "  ACTION_PROPRIO_NORMALIZATION_TYPE = bounds_q99\n",
      "If needed, manually set the correct constants in `prismatic/vla/constants.py`!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoProcessor\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfinetune\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     FinetuneConfig,\n\u001b[1;32m     15\u001b[0m     RLDSBatchTransform_epi,\n\u001b[1;32m     16\u001b[0m     PaddedCollatorForActionPrediction,\n\u001b[1;32m     17\u001b[0m     ActionTokenizer,\n\u001b[1;32m     18\u001b[0m     PurePromptBuilder,\n\u001b[1;32m     19\u001b[0m     EpisodicRLDSDataset,   \u001b[38;5;66;03m# ← PyTorch Dataset (RLDS 래핑)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m~/openvla-oft/vla-scripts/finetune.py:31\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CausalLMOutputWithPast\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexperiments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrobot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenvla_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     check_model_logic_mismatch,\n\u001b[1;32m     33\u001b[0m     model_is_on_hf_hub,\n\u001b[1;32m     34\u001b[0m     update_auto_map,\n\u001b[1;32m     35\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_prismatic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenVLAConfig\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_prismatic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenVLAForActionPrediction\n",
      "File \u001b[0;32m~/openvla-oft/experiments/robot/openvla_utils.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Apply JSON numpy patch for serialization\u001b[39;00m\n\u001b[1;32m     22\u001b[0m json_numpy\u001b[38;5;241m.\u001b[39mpatch()\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_prismatic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenVLAConfig\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_prismatic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenVLAForActionPrediction\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextern\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocessing_prismatic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PrismaticImageProcessor, PrismaticProcessor\n",
      "File \u001b[0;32m~/openvla-oft/prismatic/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m available_model_names, available_models, get_model_description, load\n",
      "File \u001b[0;32m~/openvla-oft/prismatic/models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m available_model_names, available_models, get_model_description, load, load_vla\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaterialize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_llm_backbone_and_tokenizer, get_vision_backbone_and_transform, get_vlm\n",
      "File \u001b[0;32m~/openvla-oft/prismatic/models/load.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaterialize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_llm_backbone_and_tokenizer, get_vision_backbone_and_transform\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GLOBAL_REGISTRY, MODEL_REGISTRY\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvlas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenVLA\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvlms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PrismaticVLM\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverwatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m initialize_overwatch\n",
      "File \u001b[0;32m~/openvla-oft/prismatic/models/vlas/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenvla\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenVLA\n",
      "File \u001b[0;32m~/openvla-oft/prismatic/models/vlas/openvla.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvlms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PrismaticVLM\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverwatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m initialize_overwatch\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maction_tokenizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ActionTokenizer\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Initialize Overwatch =>> Wraps `logging.Logger`\u001b[39;00m\n\u001b[1;32m     20\u001b[0m overwatch \u001b[38;5;241m=\u001b[39m initialize_overwatch(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/openvla-oft/prismatic/vla/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaterialize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_vla_dataset_and_collator\n",
      "File \u001b[0;32m~/openvla-oft/prismatic/vla/materialize.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PaddedCollatorForActionPrediction\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maction_tokenizer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ActionTokenizer\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EpisodicRLDSDataset, RLDSBatchTransform, RLDSDataset\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_vla_dataset_and_collator\u001b[39m(\n\u001b[1;32m     22\u001b[0m     data_root_dir: Path,\n\u001b[1;32m     23\u001b[0m     data_mix: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     image_aug: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     34\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dataset, ActionTokenizer, PaddedCollatorForActionPrediction]:\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize RLDS Dataset (wraps TFDS), ActionTokenizer, and initialize transform/collation functions.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/openvla-oft/prismatic/vla/datasets/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DummyDataset, EpisodicRLDSDataset, RLDSBatchTransform, RLDSDataset, EpisodicDataset, RLDSDataset_epi, RLDSBatchTransform_epi\n",
      "File \u001b[0;32m~/openvla-oft/prismatic/vla/datasets/datasets.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ACTION_DIM, ACTION_PROPRIO_NORMALIZATION_TYPE, ACTION_TOKEN_BEGIN_IDX, IGNORE_INDEX, NUM_ACTIONS_CHUNK, PROPRIO_DIM, STOP_INDEX\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrlds\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_interleaved_dataset, make_single_dataset, make_interleaved_dataset_memory\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrlds\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moxe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OXE_NAMED_MIXTURES, get_oxe_dataset_kwargs_and_weights\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMemoryGridGenerator\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tile_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m):\n",
      "File \u001b[0;32m~/openvla-oft/prismatic/vla/datasets/rlds/oxe/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmaterialize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_oxe_dataset_kwargs_and_weights\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixtures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OXE_NAMED_MIXTURES\n",
      "File \u001b[0;32m~/openvla-oft/prismatic/vla/datasets/rlds/oxe/materialize.py:14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moverwatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m initialize_overwatch\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ACTION_DIM, ACTION_PROPRIO_NORMALIZATION_TYPE, ACTION_TOKEN_BEGIN_IDX, IGNORE_INDEX, NUM_ACTIONS_CHUNK, PROPRIO_DIM, STOP_INDEX\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrlds\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moxe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OXE_DATASET_CONFIGS, ActionEncoding\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrlds\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moxe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OXE_STANDARDIZATION_TRANSFORMS\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize Overwatch =>> Wraps `logging.Logger`\u001b[39;00m\n",
      "File \u001b[0;32m~/openvla-oft/prismatic/vla/datasets/rlds/oxe/configs.py:29\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mconfigs.py\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    action_encoding: Type of action encoding (e.g., EEF Position vs. Joint Position)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprismatic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvla\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrlds\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moxe\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdroid_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m zero_action_filter\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Defines Proprioceptive State Encoding Schemes\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mStateEncoding\u001b[39;00m(IntEnum):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n",
      "File \u001b[0;32m~/openvla-oft/prismatic/vla/datasets/rlds/oxe/utils/droid_utils.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_graphics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtfg\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrmat_to_euler\u001b[39m(rot_mat):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tfg\u001b[38;5;241m.\u001b[39meuler\u001b[38;5;241m.\u001b[39mfrom_rotation_matrix(rot_mat)\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla-oft/lib/python3.10/site-packages/tensorflow_graphics/geometry/transformation/__init__.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m division\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_graphics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m axis_angle\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_graphics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dual_quaternion\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_graphics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m euler\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla-oft/lib/python3.10/site-packages/tensorflow_graphics/geometry/transformation/axis_angle.py:42\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuple\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_graphics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quaternion \u001b[38;5;28;01mas\u001b[39;00m quaternion_lib\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_graphics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rotation_matrix_3d\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_graphics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vector\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla-oft/lib/python3.10/site-packages/tensorflow_graphics/geometry/transformation/quaternion.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msix\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmoves\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mrange\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_graphics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rotation_matrix_3d\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_graphics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vector\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_graphics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m asserts\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla-oft/lib/python3.10/site-packages/tensorflow_graphics/geometry/transformation/rotation_matrix_3d.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msix\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmoves\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mrange\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_graphics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rotation_matrix_common\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_graphics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m asserts\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_graphics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_api\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla-oft/lib/python3.10/site-packages/tensorflow_graphics/geometry/transformation/rotation_matrix_common.py:77\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexpand_dims(output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# API contains all public functions and classes.\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m \u001b[43mexport_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_functions_and_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla-oft/lib/python3.10/site-packages/tensorflow_graphics/util/export_api.py:31\u001b[0m, in \u001b[0;36mget_functions_and_classes\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_functions_and_classes\u001b[39m():\n\u001b[1;32m     26\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Extracts a list of public functions and classes for the API generation.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    A list of function and class names.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m   caller \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     32\u001b[0m   module \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(caller[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     33\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     34\u001b[0m       obj_name \u001b[38;5;28;01mfor\u001b[39;00m obj_name, obj \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mgetmembers(module)\n\u001b[1;32m     35\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misfunction(obj) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m     36\u001b[0m       inspect\u001b[38;5;241m.\u001b[39misclass(obj) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m   ]\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla-oft/lib/python3.10/inspect.py:1673\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstack\u001b[39m(context\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetouterframes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla-oft/lib/python3.10/inspect.py:1650\u001b[0m, in \u001b[0;36mgetouterframes\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1648\u001b[0m framelist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m frame:\n\u001b[0;32m-> 1650\u001b[0m     frameinfo \u001b[38;5;241m=\u001b[39m (frame,) \u001b[38;5;241m+\u001b[39m \u001b[43mgetframeinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     framelist\u001b[38;5;241m.\u001b[39mappend(FrameInfo(\u001b[38;5;241m*\u001b[39mframeinfo))\n\u001b[1;32m   1652\u001b[0m     frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_back\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla-oft/lib/python3.10/inspect.py:1620\u001b[0m, in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m isframe(frame):\n\u001b[1;32m   1618\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m is not a frame or traceback object\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(frame))\n\u001b[0;32m-> 1620\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m getfile(frame)\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1622\u001b[0m     start \u001b[38;5;241m=\u001b[39m lineno \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m context\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla-oft/lib/python3.10/inspect.py:829\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[1;32m    828\u001b[0m \u001b[38;5;66;03m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n\u001b[0;32m--> 829\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mgetmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__loader__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n",
      "File \u001b[0;32m~/miniconda3/envs/openvla-oft/lib/python3.10/inspect.py:869\u001b[0m, in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;66;03m# Update the filename to module name cache and check yet again\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;66;03m# Copy sys.modules in order to cope with changes while iterating\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m modname, module \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ismodule(module) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m__file__\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    870\u001b[0m         f \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;241m==\u001b[39m _filesbymodname\u001b[38;5;241m.\u001b[39mget(modname, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    872\u001b[0m             \u001b[38;5;66;03m# Have already mapped this module, so skip it\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "# 프로젝트 루트 등록\n",
    "sys.path += [\n",
    "    \"/home/lcw/openvla-oft\",\n",
    "    \"/home/lcw/openvla-oft/vla-scripts\",\n",
    "]\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "from finetune import (\n",
    "    FinetuneConfig,\n",
    "    RLDSBatchTransform_epi,\n",
    "    PaddedCollatorForActionPrediction,\n",
    "    ActionTokenizer,\n",
    "    PurePromptBuilder,\n",
    "    EpisodicRLDSDataset,   # ← PyTorch Dataset (RLDS 래핑)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = FinetuneConfig(\n",
    "    vla_path=\"openvla/openvla-7b\",\n",
    "    data_root_dir=Path(\"/home/lcw/openvla-oft/datasets/modified_libero_rlds\"),\n",
    "    dataset_name=\"libero_10_no_noops\",\n",
    "    num_images_in_input=2,\n",
    "    use_proprio=True,\n",
    "    image_aug=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_info.json이 있는 디렉터리까지 지정\n",
    "version_dir = cfg.data_root_dir / cfg.dataset_name / \"1.0.0\"\n",
    "builder = tfds.builder_from_directory(version_dir)\n",
    "builder.download_and_prepare()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(cfg.vla_path, trust_remote_code=True)\n",
    "action_tok = ActionTokenizer(processor.tokenizer)\n",
    "\n",
    "batch_transform = RLDSBatchTransform_epi(\n",
    "    action_tokenizer = action_tok,\n",
    "    base_tokenizer   = processor.tokenizer,\n",
    "    image_transform  = processor.image_processor.apply_transform,\n",
    "    prompt_builder_fn= PurePromptBuilder,\n",
    "    use_wrist_image  = cfg.num_images_in_input > 1,\n",
    "    use_proprio      = cfg.use_proprio,\n",
    ")\n",
    "\n",
    "resize_hw = tuple(processor.image_processor.input_sizes[0][1:])  # (H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 21:52:32.702581: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">06/30 [21:52:33] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; <span style=\"font-weight: bold\">[</span>*<span style=\"font-weight: bold\">]</span> Loading existing dataset statistics from                       <a href=\"file:///home/lcw/openvla-oft/prismatic/vla/datasets/rlds/utils/data_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lcw/openvla-oft/prismatic/vla/datasets/rlds/utils/data_utils.py#201\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">201</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/home/lcw/openvla-oft/datasets/modified_libero_rlds/libero_10_no_noops/</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #800080; text-decoration-color: #800080\">1.0.0/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">dataset_statistics_f1f4cb06e922a6979da3f480f311f18217dce47441ef4b</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">608b3eaaa06c950589.json.</span>                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m06/30 [21:52:33]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> \u001b[1m[\u001b[0m*\u001b[1m]\u001b[0m Loading existing dataset statistics from                       \u001b]8;id=224396;file:///home/lcw/openvla-oft/prismatic/vla/datasets/rlds/utils/data_utils.py\u001b\\\u001b[2mdata_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=57076;file:///home/lcw/openvla-oft/prismatic/vla/datasets/rlds/utils/data_utils.py#201\u001b\\\u001b[2m201\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[35m/home/lcw/openvla-oft/datasets/modified_libero_rlds/libero_10_no_noops/\u001b[0m \u001b[2m                 \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[35m1.0.0/\u001b[0m\u001b[95mdataset_statistics_f1f4cb06e922a6979da3f480f311f18217dce47441ef4b\u001b[0m \u001b[2m                 \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[95m608b3eaaa06c950589.json.\u001b[0m                                                \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-30 21:52:33.386604: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n"
     ]
    }
   ],
   "source": [
    "from prismatic.vla.datasets.datasets import EpisodicDataset\n",
    "\n",
    "train_dataset = EpisodicDataset(\n",
    "    data_root_dir       = cfg.data_root_dir,         # \"datasets/modified_libero_rlds\"\n",
    "    data_mix            = cfg.dataset_name,          # \"libero_10_no_noops\"\n",
    "    batch_transform     = batch_transform,\n",
    "    resize_resolution   = tuple(processor.image_processor.input_sizes[0][1:]),\n",
    "    shuffle_buffer_size = cfg.shuffle_buffer_size,   # 예: 0\n",
    "    image_aug           = cfg.image_aug,             # True/False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋의 총 스텝(Step) 개수: 379\n"
     ]
    }
   ],
   "source": [
    "# train_dataset에 포함된 총 스텝의 개수를 출력합니다.\n",
    "print(f\"데이터셋의 총 스텝(Step) 개수: {len(train_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">06/30 [21:52:37] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; Failed to extract font properties from                          <a href=\"file:///home/lcw/miniconda3/envs/openvla-oft/lib/python3.10/site-packages/matplotlib/font_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">font_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lcw/miniconda3/envs/openvla-oft/lib/python3.10/site-packages/matplotlib/font_manager.py#1107\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1107</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/usr/share/fonts/truetype/noto/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">NotoColorEmoji.ttf</span>: Can not load face <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span>         <span style=\"font-weight: bold\">(</span>unknown file format; error code <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x2</span><span style=\"font-weight: bold\">)</span>                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                    </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m06/30 [21:52:37]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> Failed to extract font properties from                          \u001b]8;id=90364;file:///home/lcw/miniconda3/envs/openvla-oft/lib/python3.10/site-packages/matplotlib/font_manager.py\u001b\\\u001b[2mfont_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=64694;file:///home/lcw/miniconda3/envs/openvla-oft/lib/python3.10/site-packages/matplotlib/font_manager.py#1107\u001b\\\u001b[2m1107\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[35m/usr/share/fonts/truetype/noto/\u001b[0m\u001b[95mNotoColorEmoji.ttf\u001b[0m: Can not load face \u001b[2m                    \u001b[0m\n",
       "\u001b[2;36m                 \u001b[0m         \u001b[1m(\u001b[0munknown file format; error code \u001b[1;36m0x2\u001b[0m\u001b[1m)\u001b[0m                                \u001b[2m                    \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> | &gt;&gt; generated new fontManager                                       <a href=\"file:///home/lcw/miniconda3/envs/openvla-oft/lib/python3.10/site-packages/matplotlib/font_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">font_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/lcw/miniconda3/envs/openvla-oft/lib/python3.10/site-packages/matplotlib/font_manager.py#1639\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1639</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m | >> generated new fontManager                                       \u001b]8;id=188705;file:///home/lcw/miniconda3/envs/openvla-oft/lib/python3.10/site-packages/matplotlib/font_manager.py\u001b\\\u001b[2mfont_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=102177;file:///home/lcw/miniconda3/envs/openvla-oft/lib/python3.10/site-packages/matplotlib/font_manager.py#1639\u001b\\\u001b[2m1639\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def visualize_grid_from_step(step, title):\n",
    "    grid_tensor = step[\"pixel_values\"]  # shape: (N, 3, H, W)\n",
    "    grid_tensor = grid_tensor.reshape(-1, 3, 224, 224)\n",
    "    imgs_pil = [TF.to_pil_image(img) for img in grid_tensor]\n",
    "    \n",
    "    fig, axs = plt.subplots(1, len(imgs_pil), figsize=(12, 3))\n",
    "    for i, img in enumerate(imgs_pil):\n",
    "        axs[i].imshow(img)\n",
    "        axs[i].set_title(f\"Img {i}\")\n",
    "        axs[i].axis(\"off\")\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = PaddedCollatorForActionPrediction(\n",
    "    processor.tokenizer.model_max_length,   # max_length\n",
    "    processor.tokenizer.pad_token_id,       # pad_token_id\n",
    "    \"right\"                                 # padding_side\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size  = 4,\n",
    "    shuffle     = False,\n",
    "    collate_fn  = collator,\n",
    "    num_workers = 0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ deque를 전역으로 추가했습니다.\n",
      "deque 추가 후 재테스트:\n",
      "[EPISODE 1] Processing 190 steps\n",
      "[FIRST EPISODE] Starting episode 1 with task: 'pick up the book and place it in the back compartm...'\n",
      "[EPISODE TRANSITION] None -> libero_10_no_noops_ep_1\n",
      "[STEP 10] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 20] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 30] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 40] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 50] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 60] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 70] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 80] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 90] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 100] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 110] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 120] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 130] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 140] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 150] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 160] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 170] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 180] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 190] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[EPISODE 2] Processing 177 steps\n",
      "[STEP 200] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 210] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 220] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 230] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 240] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 250] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 260] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 270] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 280] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 290] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 300] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 310] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 320] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 330] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 340] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 350] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 360] Episode libero_10_no_noops_ep_1: 8 primary + 8 wrist → dual memory grid\n",
      "[EPISODE 3] Processing 293 steps\n",
      "[NEW EPISODE 2] Task changed at global step 368\n",
      "  From: 'pick up the book and place it in the back compartm...'\n",
      "  To:   'put the yellow and white mug in the microwave and ...'\n",
      "[EPISODE TRANSITION] libero_10_no_noops_ep_1 -> libero_10_no_noops_ep_2\n",
      "[PREV EPISODE] libero_10_no_noops_ep_1 had 8 images\n",
      "✅ 첫 번째 아이템 성공!\n",
      "first_item 타입: <class 'list'>\n",
      "first_item 길이: 367\n",
      "리스트의 첫 번째 요소 타입: <class 'dict'>\n",
      "첫 번째 요소의 키들: ['pixel_values', 'input_ids', 'labels', 'dataset_name', 'actions', 'episode_id', 'step_in_episode', 'global_step', 'task_instruction', 'proprio']\n",
      "Dataset name: b'libero_10_no_noops'\n"
     ]
    }
   ],
   "source": [
    "# 전역에 deque를 추가해서 모든 모듈에서 사용 가능하게 만들기\n",
    "import builtins\n",
    "from collections import deque\n",
    "\n",
    "builtins.deque = deque\n",
    "\n",
    "print(\"✅ deque를 전역으로 추가했습니다.\")\n",
    "\n",
    "# first_item의 타입과 구조 확인\n",
    "print(\"deque 추가 후 재테스트:\")\n",
    "try:\n",
    "    dataset_iter = iter(train_dataset)\n",
    "    \n",
    "    first_item = next(dataset_iter)\n",
    "    print(f\"first_item 타입: {type(first_item)}\")\n",
    "    print(f\"first_item 길이: {len(first_item) if hasattr(first_item, '__len__') else 'N/A'}\")\n",
    "    \n",
    "    # list라면 첫 번째 요소 확인\n",
    "    if isinstance(first_item, list):\n",
    "        print(f\"리스트의 첫 번째 요소 타입: {type(first_item[0])}\")\n",
    "        if hasattr(first_item[0], 'keys'):\n",
    "            print(f\"첫 번째 요소의 키들: {list(first_item[0].keys())}\")\n",
    "            print(f\"Dataset name: {first_item[0].get('dataset_name', 'no name')}\")\n",
    "        else:\n",
    "            print(f\"첫 번째 요소: {first_item[0]}\")\n",
    "    \n",
    "    # dictionary라면 직접 접근\n",
    "    elif hasattr(first_item, 'keys'):\n",
    "        print(f\"Dictionary 키들: {list(first_item.keys())}\")\n",
    "        print(f\"Dataset name: {first_item.get('dataset_name', 'no name')}\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"예상과 다른 타입입니다: {first_item}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 오류: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPISODE 1] Processing 237 steps\n",
      "[NEW EPISODE 3] Task changed at global step 369\n",
      "  From: 'put the yellow and white mug in the microwave and ...'\n",
      "  To:   'pick up the book and place it in the back compartm...'\n",
      "[EPISODE TRANSITION] libero_10_no_noops_ep_2 -> libero_10_no_noops_ep_3\n",
      "[PREV EPISODE] libero_10_no_noops_ep_2 had 1 images\n",
      "[STEP 10] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 20] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 30] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 40] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 50] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 60] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 70] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 80] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 90] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 100] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 110] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 120] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 130] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 140] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 150] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 160] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 170] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 180] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 190] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 200] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 210] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 220] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 230] Episode libero_10_no_noops_ep_3: 8 primary + 8 wrist → dual memory grid\n",
      "[EPISODE 2] Processing 230 steps\n",
      "[NEW EPISODE 4] Task changed at global step 606\n",
      "  From: 'pick up the book and place it in the back compartm...'\n",
      "  To:   'put the white mug on the left plate and put the ye...'\n",
      "[EPISODE TRANSITION] libero_10_no_noops_ep_3 -> libero_10_no_noops_ep_4\n",
      "[PREV EPISODE] libero_10_no_noops_ep_3 had 8 images\n",
      "[STEP 10] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 20] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 30] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 40] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 50] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 60] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 70] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 80] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 90] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 100] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 110] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 120] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 130] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 140] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 150] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 160] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 170] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 180] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 190] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 200] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 210] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 220] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 230] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[EPISODE 3] Processing 235 steps\n",
      "[STEP 240] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 250] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 260] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 270] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 280] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 290] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 300] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 310] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 320] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 330] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 340] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 350] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 360] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 370] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 380] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 390] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 400] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 410] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 420] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 430] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 440] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 450] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 460] Episode libero_10_no_noops_ep_4: 8 primary + 8 wrist → dual memory grid\n",
      "[EPISODE 4] Processing 265 steps\n",
      "[NEW EPISODE 5] Task changed at global step 1071\n",
      "  From: 'put the white mug on the left plate and put the ye...'\n",
      "  To:   'put both the cream cheese box and the butter in th...'\n",
      "[EPISODE TRANSITION] libero_10_no_noops_ep_4 -> libero_10_no_noops_ep_5\n",
      "[PREV EPISODE] libero_10_no_noops_ep_4 had 8 images\n",
      "[STEP 10] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 20] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 30] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 40] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 50] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 60] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 70] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 80] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 90] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 100] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 110] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 120] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 130] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 140] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 150] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 160] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 170] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 180] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 190] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 200] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 210] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 220] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 230] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 240] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 250] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 260] Episode libero_10_no_noops_ep_5: 8 primary + 8 wrist → dual memory grid\n",
      "[EPISODE 5] Processing 218 steps\n",
      "[NEW EPISODE 6] Task changed at global step 1336\n",
      "  From: 'put both the cream cheese box and the butter in th...'\n",
      "  To:   'put the white mug on the left plate and put the ye...'\n",
      "[EPISODE TRANSITION] libero_10_no_noops_ep_5 -> libero_10_no_noops_ep_6\n",
      "[PREV EPISODE] libero_10_no_noops_ep_5 had 8 images\n",
      "[CLEANUP] Removing old episode: libero_10_no_noops_ep_1\n",
      "[CLEANUP] Removing old episode: libero_10_no_noops_ep_2\n",
      "[STEP 10] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 20] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 30] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 40] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 50] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 60] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 70] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 80] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 90] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 100] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 110] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 120] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 130] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 140] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 150] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 160] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 170] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 180] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 190] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 200] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 210] Episode libero_10_no_noops_ep_6: 8 primary + 8 wrist → dual memory grid\n",
      "[EPISODE 6] Processing 221 steps\n",
      "[NEW EPISODE 7] Task changed at global step 1554\n",
      "  From: 'put the white mug on the left plate and put the ye...'\n",
      "  To:   'put the white mug on the plate and put the chocola...'\n",
      "[EPISODE TRANSITION] libero_10_no_noops_ep_6 -> libero_10_no_noops_ep_7\n",
      "[PREV EPISODE] libero_10_no_noops_ep_6 had 8 images\n",
      "[STEP 10] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 20] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 30] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 40] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 50] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 60] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 70] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 80] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 90] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 100] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 110] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 120] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 130] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 140] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 150] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 160] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 170] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 180] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 190] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 200] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 210] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 220] Episode libero_10_no_noops_ep_7: 8 primary + 8 wrist → dual memory grid\n",
      "[EPISODE 7] Processing 247 steps\n",
      "[NEW EPISODE 8] Task changed at global step 1775\n",
      "  From: 'put the white mug on the plate and put the chocola...'\n",
      "  To:   'put both the cream cheese box and the butter in th...'\n",
      "[EPISODE TRANSITION] libero_10_no_noops_ep_7 -> libero_10_no_noops_ep_8\n",
      "[PREV EPISODE] libero_10_no_noops_ep_7 had 8 images\n",
      "[CLEANUP] Removing old episode: libero_10_no_noops_ep_3\n",
      "[CLEANUP] Removing old episode: libero_10_no_noops_ep_4\n",
      "[STEP 10] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 20] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 30] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 40] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 50] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 60] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 70] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 80] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 90] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 100] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 110] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 120] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 130] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 140] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 150] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 160] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 170] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 180] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 190] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 200] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 210] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 220] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 230] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 240] Episode libero_10_no_noops_ep_8: 8 primary + 8 wrist → dual memory grid\n",
      "[EPISODE 8] Processing 207 steps\n",
      "[NEW EPISODE 9] Task changed at global step 2022\n",
      "  From: 'put both the cream cheese box and the butter in th...'\n",
      "  To:   'put the white mug on the left plate and put the ye...'\n",
      "[EPISODE TRANSITION] libero_10_no_noops_ep_8 -> libero_10_no_noops_ep_9\n",
      "[PREV EPISODE] libero_10_no_noops_ep_8 had 8 images\n",
      "[STEP 10] Episode libero_10_no_noops_ep_9: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 20] Episode libero_10_no_noops_ep_9: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 30] Episode libero_10_no_noops_ep_9: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 40] Episode libero_10_no_noops_ep_9: 8 primary + 8 wrist → dual memory grid\n",
      "[STEP 50] Episode libero_10_no_noops_ep_9: 8 primary + 8 wrist → dual memory grid\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Modify the dataset to return individual items instead of lists\n",
    "# Add this to your dataset creation/loading code:\n",
    "\n",
    "def flatten_dataset_items(dataset):\n",
    "    \"\"\"Convert dataset that returns lists of items to individual items\"\"\"\n",
    "    for item_list in dataset:\n",
    "        if isinstance(item_list, list):\n",
    "            for individual_item in item_list:\n",
    "                yield individual_item\n",
    "        else:\n",
    "            yield item_list\n",
    "\n",
    "# Apply the flattening before creating DataLoader:\n",
    "flattened_dataset = list(flatten_dataset_items(train_dataset))\n",
    "print(f\"Flattened dataset length: {len(flattened_dataset)}\")\n",
    "\n",
    "# Create a simple wrapper dataset\n",
    "class FlattenedDataset:\n",
    "    def __init__(self, items):\n",
    "        self.items = items\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.items[idx]\n",
    "\n",
    "# Use the flattened dataset\n",
    "flat_dataset = FlattenedDataset(flattened_dataset)\n",
    "loader = DataLoader(flat_dataset, batch_size=1, collate_fn=collate_fn)\n",
    "\n",
    "# Option 2: Modify the collate function to handle list inputs\n",
    "# In your data_utils.py, modify the _find_episode_pairs method:\n",
    "\n",
    "def _find_episode_pairs(self, instances):\n",
    "    \"\"\"Find pairs of instances that belong to different episodes.\"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    # Handle case where instances might be lists\n",
    "    flat_instances = []\n",
    "    for item in instances:\n",
    "        if isinstance(item, list):\n",
    "            flat_instances.extend(item)\n",
    "        else:\n",
    "            flat_instances.append(item)\n",
    "    \n",
    "    # Group by episode\n",
    "    episodes = {}\n",
    "    for item in flat_instances:\n",
    "        episode_id = item.get('episode_id', 'unknown')\n",
    "        if episode_id not in episodes:\n",
    "            episodes[episode_id] = []\n",
    "        episodes[episode_id].append(item)\n",
    "    \n",
    "    # Continue with existing logic...\n",
    "    episode_keys = list(episodes.keys())\n",
    "    for i in range(len(episode_keys)):\n",
    "        for j in range(i + 1, len(episode_keys)):\n",
    "            ep1_items = episodes[episode_keys[i]]\n",
    "            ep2_items = episodes[episode_keys[j]]\n",
    "            pairs.extend([(item1, item2) for item1 in ep1_items for item2 in ep2_items])\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Option 3: Quick test to verify the fix works\n",
    "def test_dataloader_fix():\n",
    "    try:\n",
    "        # Test with the first approach\n",
    "        dataset_iter = iter(train_dataset)\n",
    "        raw_item = next(dataset_iter)\n",
    "        \n",
    "        if isinstance(raw_item, list):\n",
    "            print(f\"Raw item is list with {len(raw_item)} elements\")\n",
    "            # Test individual items\n",
    "            for i, item in enumerate(raw_item[:3]):\n",
    "                print(f\"Item {i}: episode_id = {item.get('episode_id', 'N/A')}\")\n",
    "                print(f\"Item {i}: step_in_episode = {item.get('step_in_episode', 'N/A')}\")\n",
    "        \n",
    "        # Create flattened version\n",
    "        flattened_items = []\n",
    "        for batch in train_dataset:\n",
    "            if isinstance(batch, list):\n",
    "                flattened_items.extend(batch)\n",
    "            else:\n",
    "                flattened_items.append(batch)\n",
    "        \n",
    "        print(f\"Total flattened items: {len(flattened_items)}\")\n",
    "        \n",
    "        # Test a small batch\n",
    "        test_items = flattened_items[:4]  # Small batch for testing\n",
    "        test_batch = collate_fn(test_items)\n",
    "        print(\"Collation successful!\")\n",
    "        \n",
    "        return flattened_items\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run the test\n",
    "flattened_items = test_dataloader_fix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_transform 디버깅\n",
    "print(\"🔧 batch_transform 디버깅:\")\n",
    "try:\n",
    "    # 원본 RLDS에서 step 하나 가져오기\n",
    "    ds = builder.as_dataset(split='train')\n",
    "    \n",
    "    for episode in ds.take(1):\n",
    "        for i, step in enumerate(episode['steps'].take(1)):\n",
    "            print(f\"\\n=== Step {i} 원본 분석 ===\")\n",
    "            print(f\"원본 step 키들: {list(step.keys())}\")\n",
    "            \n",
    "            # observation 내부 구조 확인\n",
    "            obs = step['observation']\n",
    "            print(f\"Observation 키들: {list(obs.keys())}\")\n",
    "            \n",
    "            # 각 키의 상세 정보\n",
    "            for key, val in step.items():\n",
    "                if key == 'observation':\n",
    "                    for obs_key, obs_val in obs.items():\n",
    "                        if hasattr(obs_val, 'shape'):\n",
    "                            print(f\"  observation.{obs_key}: {obs_val.shape} {obs_val.dtype}\")\n",
    "                elif hasattr(val, 'shape'):\n",
    "                    print(f\"  {key}: {val.shape} {val.dtype}\")\n",
    "                elif hasattr(val, 'numpy'):\n",
    "                    val_np = val.numpy()\n",
    "                    if isinstance(val_np, bytes):\n",
    "                        print(f\"  {key}: '{val_np.decode('utf-8')}'\")\n",
    "                    else:\n",
    "                        print(f\"  {key}: {val_np}\")\n",
    "                else:\n",
    "                    print(f\"  {key}: {type(val)}\")\n",
    "            \n",
    "            # step을 Python dict로 변환\n",
    "            print(f\"\\n=== Step 변환 과정 ===\")\n",
    "            step_dict = {}\n",
    "            for key, val in step.items():\n",
    "                if key == 'observation':\n",
    "                    step_dict[key] = {obs_key: obs_val.numpy() if hasattr(obs_val, 'numpy') else obs_val \n",
    "                                     for obs_key, obs_val in obs.items()}\n",
    "                else:\n",
    "                    step_dict[key] = val.numpy() if hasattr(val, 'numpy') else val\n",
    "            \n",
    "            print(f\"변환된 step_dict 키들: {list(step_dict.keys())}\")\n",
    "            print(f\"observation 내부: {list(step_dict['observation'].keys())}\")\n",
    "            \n",
    "            # episode_metadata에서 정보 추출\n",
    "            episode_metadata = episode['episode_metadata']\n",
    "            print(f\"\\nEpisode metadata 키들: {list(episode_metadata.keys())}\")\n",
    "            \n",
    "            # episode_id 추가 (만약 없다면)\n",
    "            if 'episode_id' not in step_dict:\n",
    "                # episode_metadata에서 file_path나 다른 정보로 episode_id 생성\n",
    "                if 'file_path' in episode_metadata:\n",
    "                    file_path = episode_metadata['file_path'].numpy().decode('utf-8')\n",
    "                    episode_id = file_path.split('/')[-1]  # 파일명을 episode_id로 사용\n",
    "                    step_dict['episode_id'] = episode_id\n",
    "                    print(f\"생성된 episode_id: {episode_id}\")\n",
    "                else:\n",
    "                    step_dict['episode_id'] = f\"episode_{i}\"\n",
    "                    print(f\"기본 episode_id: episode_{i}\")\n",
    "            \n",
    "            # dataset_name 추가\n",
    "            step_dict['dataset_name'] = cfg.dataset_name\n",
    "            \n",
    "            print(f\"\\n=== batch_transform 적용 ===\")\n",
    "            # 이제 batch_transform 적용\n",
    "            try:\n",
    "                transformed = batch_transform(step_dict)\n",
    "                print(f\"✅ Transform 성공!\")\n",
    "                print(f\"Transform 결과 타입: {type(transformed)}\")\n",
    "                \n",
    "                if isinstance(transformed, dict):\n",
    "                    print(f\"Transform 결과 키들: {list(transformed.keys())}\")\n",
    "                    for key, val in transformed.items():\n",
    "                        if hasattr(val, 'shape'):\n",
    "                            print(f\"  {key}: {val.shape}\")\n",
    "                        else:\n",
    "                            print(f\"  {key}: {type(val)}\")\n",
    "                elif isinstance(transformed, list):\n",
    "                    print(f\"⚠️  Transform이 list를 반환했습니다! 길이: {len(transformed)}\")\n",
    "                    if len(transformed) > 0:\n",
    "                        print(f\"첫 번째 요소 타입: {type(transformed[0])}\")\n",
    "                        if hasattr(transformed[0], 'keys'):\n",
    "                            print(f\"첫 번째 요소 키들: {list(transformed[0].keys())}\")\n",
    "                \n",
    "            except Exception as transform_error:\n",
    "                print(f\"❌ Transform 오류: {transform_error}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            break  # 첫 번째 step만 분석\n",
    "        break  # 첫 번째 episode만 분석\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 전체 디버깅 실패: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 재시도\n",
    "print(\"\\n🚀 DataLoader 테스트:\")\n",
    "try:\n",
    "    loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=collator,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    \n",
    "    batch = next(iter(loader))\n",
    "    print(\"✅ DataLoader 성공!\")\n",
    "    \n",
    "    print(f\"배치 타입: {type(batch)}\")\n",
    "    if hasattr(batch, 'keys'):\n",
    "        print(f\"배치 키들: {list(batch.keys())}\")\n",
    "        for key, val in batch.items():\n",
    "            if hasattr(val, \"shape\"):\n",
    "                print(f\"  {key}: {val.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ DataLoader 실패: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in batch.items():\n",
    "    if hasattr(val, \"shape\"):\n",
    "        print(f\"{key:20s} → {tuple(val.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "imgs = batch[\"pixel_values\"][3]\n",
    "imgs = imgs.reshape(-1, 3, 224, 224)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i, img_t in enumerate(imgs):\n",
    "    img_pil = TF.to_pil_image(img_t)\n",
    "    axs[i].imshow(img_pil)\n",
    "    axs[i].set_title(f\"Cam {i}\")\n",
    "    axs[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 이미지 추출 (4개 카메라 뷰)\n",
    "imgs = batch[\"pixel_values\"][3]          # (12, 224, 224)\n",
    "imgs = imgs.reshape(-1, 3, 224, 224)     # (4, 3, 224, 224)\n",
    "\n",
    "# 2. CLIP 준비\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").eval().cuda()\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# 3. PIL 변환 + CLIP 입력 변환\n",
    "imgs_pil = [TF.to_pil_image(img) for img in imgs]\n",
    "clip_inputs = clip_processor(images=imgs_pil, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# 4. CLIP embedding 추출 + norm 계산\n",
    "with torch.no_grad():\n",
    "    clip_embeds = clip_model.get_image_features(**clip_inputs)  # (4, 512)\n",
    "\n",
    "norms = torch.norm(clip_embeds, dim=-1)  # (4,)\n",
    "print(\"📊 Norms:\", norms.tolist())\n",
    "\n",
    "# 5. Norm threshold로 마스킹\n",
    "threshold = 5.0\n",
    "mask = norms > threshold  # BoolTensor of shape (4,)\n",
    "print(\"Keep mask:\", mask.tolist())\n",
    "\n",
    "# 6. 마스킹 후 유효 이미지만 추출\n",
    "valid_imgs = [img for img, keep in zip(imgs_pil, mask) if keep]\n",
    "\n",
    "# 7. 시각화\n",
    "fig, axs = plt.subplots(1, len(imgs_pil), figsize=(12, 3))\n",
    "for i, (img_pil, norm_val) in enumerate(zip(imgs_pil, norms)):\n",
    "    axs[i].imshow(img_pil)\n",
    "    axs[i].set_title(f\"Cam {i}\\nNorm: {norm_val:.2f}\")\n",
    "    if not mask[i]:\n",
    "        axs[i].spines['bottom'].set_color('red')  # 제거된 뷰 강조\n",
    "        axs[i].spines['bottom'].set_linewidth(3)\n",
    "    axs[i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. memory grid 만들 때 valid_imgs만 사용\n",
    "# memory_img, memory_mask = generator.make_memory_grid(valid_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvla-oft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
